---
title: Introduction
description: Introduction to mllm
prev: false
---

**mllm** is a fast multimodal LLM inference engine for mobile and edge devices, mainly supporting CPU inference on Android devices, and also supports accelerating inference through GPU and NPU methods. 

Currently, mllm supports the inference of large models like <ins>llama</ins>, <ins>fuyu</ins>, <ins>vit</ins>, <ins>imagebind</ins>, <ins>clip</ins>, etc.

## Key Features


## Give it a try
mllm provides a series of [example programs](https://github.com/UbiquitousLearning/mllm/examples), including the implementation of llama, clip, fuyu, vit, imagebind, and more using the mllm framework. 

In addition, mllm also offers [an example app](https://github.com/UbiquitousLearning/mllm/android) for Android devices, where you can upload models to your phone via adb to experience the effects of different models' inference on mllm.

