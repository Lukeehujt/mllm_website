---
title: 添加新模型
sidebar:
 order: 3
---


:::tip
在添加新模型前，请参阅[模型列表文档]()，避免不必要的重复。
:::
添加Op包含如下步骤：
1. （可选）[增加新的Tokenizer](#增加新的Tokenizer)
1. （可选）[增加新的PreProcessor](#增加新的PreProcessor)
2. [搭建模型结构](#搭建模型结构)
2. [添加PostProcessing](#添加PostProcessing)
3. [构建处理流程](#构建处理流程)

##  增加新的Tokenizer

需要查阅[tokenizer目录](https://github.com/UbiquitousLearning/mllm/tree/develop/src/tokenizers), 检查您想要添加的模型需要的tokenizer是否已经支持，如果已经支持，请跳转至下一章节。

1. 在[tokenizer目录](https://github.com/UbiquitousLearning/mllm/tree/develop/src/tokenizers)下添加XXX/XXXTokenizer.hpp、XXX/XXXTokenizer.cpp。
2. 类声明。 继承基类Tokenizer，主要实现tokenize()函数。

示例如下：
```cpp
#ifndef MLLM_XXX_HPP
#define MLLM_XXX_HPP
#include "tokenizers/Tokenizer.hpp"
namespace mllm {
class XXXTokenizer final : public Tokenizer {
    ... ...,

public:
    vector<std::string> XXX(const std::string &token);
    void tokenize(const std::string &text, std::vector<token_id_t> &tokens, bool bos) override;
};
} // namespace mllm

#endif // MLLM_XXX_HPP
```

##  增加新的PreProcessor

需要查阅[processor目录](https://github.com/UbiquitousLearning/mllm/tree/develop/src/processor), 检查您想要添加的模型需要的PreProcessor需要的功能是否已经支持，如果已经支持，请跳转至下一章节。尽量不要实现新的PreProcessor。

1. 在[processor目录](https://github.com/UbiquitousLearning/mllm/tree/develop/src/processor)下添加XXXPreProcess.hpp、XXXPreProcess.cpp。
2. 类声明。 继承基类PreProcessor，主要重写Process()函数。

示例如下：
```cpp
#ifndef MLLM_XXX_HPP
#define MLLM_XXX_HPP
#include "PreProcess.hpp"
namespace mllm {
class XXXProcessor:public mllm::PreProcessor {
public:
    explicit XXXProcessor(mllm::Tokenizer *tokenizer, int height, int width, bool do_pad, bool do_resize, bool do_normalize, bool do_rescale, std::vector<float> mean) :
        PreProcessor(tokenizer, height, width, do_pad, do_resize, do_normalize, do_rescale, std::move(mean), std::move(std)) {
    }
    void Process(const std::string &text) override;
    void PreProcessImages(const std::vector<std::string> &images_path) override;

};
} // namespace mllm

#endif // MLLM_XXX_HPP
```

## 搭建模型结构

**在[example目录](https://github.com/UbiquitousLearning/mllm/tree/develop/examples)下新建main_XXXModel.cpp文件，并在[CMakeList.txt](https://github.com/UbiquitousLearning/mllm/blob/develop/CMakeLists.txt)中构建之。**

请参考[example](https://github.com/UbiquitousLearning/mllm/tree/develop/examples)和[Op列表文档]()，搭建模型结构。

例如构建CLIP的模型：
```cpp
... ...

void CLIP(Context* c) {
    auto *i = _Input(c, {}, "input_ids");
    i = transformer(c, i);
    auto *p = _Input(c, {}, "input_imgs");
    p = vit(c, p);
    i = _Linear( {i}, 512, 512, false, "text_projection");
    i = *i/i->norm(2);
    p = _Linear( {p}, 768, 512, false, "visual_projection");
    p = *p/p->norm(2);
    auto *o = _Matmul( {i, p}, false, true, "matmul");
    o = _Scale( {o}, 100.0, 0.0F, false, "scale");
}
... ...
```

## 添加PostProcessing
在main_XXXModel.cpp中添加PostProcessing函数。
此函数用于处理模型的输出。
例如：

```cpp
vector<float> postProcessing(shared_ptr<Tensor> result){
    vector<float> scores;
    for (int i = 0; i < result->batch(); ++i) {
        auto value = result->dataAt<float>(i, 0, 0, 0);
        scores.push_back(value);
    }
    auto token_idx =  softmax(scores);
    return token_idx;
}
```

## 构建处理流程

在main_XXXModel.cpp的main()中按照如下流程进行处理。

1. 新建Context，构建模型。
```cpp
std::unique_ptr<Context> c_ptr(new Context());
auto *c = c_ptr.get();
CLIP(c);
```

2. 选择Backend，从Context构建Net。
```cpp
BackendConfig bn;
Net net(bn);
net.convert(c->sub_param_);
```

3. 构建Executor，加载模型权重。
```cpp
string model_path = "model.ckpt.mllm";
ParamLoader param_loader(model_path);
Executor ex(&param_loader);
ex.setup(&net);
```

4. Tokenizer根据词表对输入sequences处理。
```cpp
string vocab_path = "vocab.mllm";
auto tokenizer = new BPETokenizer(vocab_path);
... ...

vector<string> in_strs = {"a photo of a cat"};
auto tokens_ids = vector<vector<token_id_t>>();
for (auto in_str : in_strs) {
    vector<mllm::token_id_t> tokens_id={};
    tokenizer->tokenize(in_str, tokens_id, true);
        tokens_ids.push_back(tokens_id);
}
shared_ptr<Tensor> input_text = std::make_shared<Tensor>();
BPETokenizer::tokens2Tensor(&net, tokens_ids, input_text);
```

5. (可选)Processor处理图片等其他模态输入。
```cpp
shared_ptr<Tensor> input_img = std::make_shared<Tensor>();
auto* processor = new ClipProcessor(tokenizer);
processor->PreProcessImages({"cat.jpg"});
auto images = processor->pixel_values_[0];
img2Tensor(input_img, net, images);
```

6. 进行模型推理。注意输入顺序需要与[搭建模型结构](#搭建模型结构)时的`_Input`顺序相同！！
```cpp
ex.run(&net, {input_text, input_img});
auto result = ex.result();

7. 进行postProcessing。
```cpp
auto probs = postProcessing(result[0]);
```